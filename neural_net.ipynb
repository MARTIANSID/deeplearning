{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "\n",
    "  # _ op tells us wich operation created this value\n",
    "  #_children contains konse 2 elements ke beech operation ka result hai yeh no\n",
    "  def __init__(self,data, _children = (), _op = '') -> None: # like a constructor\n",
    "    self.data = data\n",
    "    self._backward = lambda : None # backward is a function which will be called once the entire neural net has been made\n",
    "    self.grad = 0.0 # this is actually the derivative of final expression with respect to current value (intitally it is zero that means this value has no effect on the final result)\n",
    "    self._prev = set(_children)\n",
    "    self._op = _op\n",
    "\n",
    "\n",
    "  def __repr__(self) -> str: # used to define string representation of an object\n",
    "    return f\"Value(data={self.data})\"\n",
    "\n",
    "  def __add__(self,other):\n",
    "    out = Value(self.data + other.data,(self,other),'+')\n",
    "\n",
    "    def _backward(): ## it is a closure function and captures all the local variables that is it will capture the reference of out, self and every other object defined here\n",
    "      self.grad += 1.0*out.grad\n",
    "      other.grad += 1.0*out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "\n",
    "  def __mul__(self,other):\n",
    "    out = Value(self.data * other.data, (self,other),'*')\n",
    "\n",
    "    def _backward():\n",
    "      self.grad += other.data * out.grad\n",
    "      other.grad += self.data * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "\n",
    "  def tanh(self):\n",
    "    n = self.data\n",
    "    t = (math.exp(2*n) - 1) / (math.exp(2*n)+1) # this is tanh formula\n",
    "    print(t)\n",
    "    out = Value(t,(self,),'tanh')\n",
    "\n",
    "    def _backward():\n",
    "      self.grad += (1-t**2) * out.grad\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "  def exp(self):\n",
    "    x = self.data\n",
    "    out = Value(math.exp(x),(self,),'exp')\n",
    "\n",
    "    def _backward():\n",
    "      self.grad += out.data*out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "\n",
    "  def __truediv__(self,other):\n",
    "    return self * other**-1\n",
    "\n",
    "  def __neg__(self):\n",
    "    return self* Value(-1)\n",
    "\n",
    "  def __sub__(self,other):\n",
    "    return self + (-other)\n",
    "\n",
    "\n",
    "  def __pow__(self,other):\n",
    "    out = Value(self.data**other,(self,),f\"**{other}\") # we here assume other is of type integer or float\n",
    "\n",
    "    def _backward():\n",
    "      self.grad  += other * (self.data ** (other - 1)) * out.grad\n",
    "\n",
    "    self._backward = _backward\n",
    "\n",
    "  def topo(val):\n",
    "    topo = []\n",
    "    visited = set()\n",
    "    que = [val]\n",
    "\n",
    "    while len(que) > 0:\n",
    "      size = len(que)\n",
    "      while size > 0:\n",
    "        size-=1\n",
    "        v = que.pop(0) # we can improve the tc by implementing a que\n",
    "        if v in visited:\n",
    "          continue\n",
    "        visited.add(v)\n",
    "        topo.append(v)\n",
    "        for child in v._prev:\n",
    "          que.append(child)\n",
    "    for node in topo:\n",
    "      node._backward()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# backpropogation\n",
    "# we want to find out the derivative of final expression with respect to each value (weights) in the neural nets, this will tell us how each value (weights) influences the final outcome.\n",
    "\n",
    "# The\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2 0.500000465559325\n"
     ]
    }
   ],
   "source": [
    "# lets try the same implementation with torch library\n",
    "# tensors are nothing but 2 d arrays\n",
    "# torch simplifies the whole process and is much more optimized\n",
    "x1 = torch.tensor([2.0]).double()\n",
    "x2 = torch.tensor([0.0]).double()\n",
    "w1 = torch.tensor([-3.0]).double()\n",
    "w2 = torch.tensor([1.0]).double()\n",
    "b = torch.tensor([6.881373]).double()\n",
    "\n",
    "x1.requires_grad = True\n",
    "x2.requires_grad = True\n",
    "w1.requires_grad = True\n",
    "w2.requires_grad = True\n",
    "\n",
    "\n",
    "n = x1*w1 + x2*w2 + b\n",
    "\n",
    "o = torch.tanh(n)\n",
    "o.backward() # backpropogation\n",
    "\n",
    "print('x2',x2.grad.item())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9983278881817537\n",
      "Value(data=-0.9983278881817537)\n"
     ]
    }
   ],
   "source": [
    "class Neuron:\n",
    "  def __init__(self,nin): # nin basically tells us how many inputs comes to this Neuron\n",
    "    self.w = [Value(random.uniform(-1.0,1.0)) for _ in range(nin)] # for each input we have one corresponding weight\n",
    "    self.b = Value(random.uniform(-1.0,1.0)) # each neuron will have its own bias\n",
    "\n",
    "  def __call__(self,x): # this method is called directly from object variables like for ex obj = Neuron, then it will be called as obj(x)\n",
    "    activation = sum([w*x for w, x in zip(self.w,x)],self.b)\n",
    "    out = activation.tanh()\n",
    "    return out\n",
    "\n",
    "x = [Value(2.0),Value(3.0)]\n",
    "n = Neuron(2)\n",
    "print(n(x))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
